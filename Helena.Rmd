---
title: "Helena"
output: html_document
---

```{r r_setup, include = FALSE}
## initial settings
knitr::opts_chunk$set(
  comment = NA,
  echo = TRUE,
  error = TRUE,
  cache = FALSE,
  message = FALSE,
  dpi = 96,
  warning = FALSE
)

## width to use when printing tables etc.
options(
  width = 250,
  scipen = 100,
  max.print = 5000,
  stringsAsFactors = FALSE
)

## load radiant packages if neededi
if (!exists("r_environment")) library(radiant)
```

<style>
.table {
  width: auto;
}
ul, ol {
  padding-left: 18px;
}
pre, code, pre code {
  overflow: auto;
  white-space: pre;
  word-wrap: normal;
  background-color: #ffffff;
}
</style>

## Setup

Please complete this R-markdown document by answering the questions in `s-mobile.pdf` on Dropbox (week9/readings/). The code block below will load the data you will need. Please DO NOT change the code used to load the data. Create an HTML file with all your results and comments and push both the Rmarkdown and HTML file to GitLab when you are done. All analysis results MUST be reproducible (i.e., the TA and I must be able to recreate the HTML from the R-markdown file without changes or errors). Upload all files to GitLab.

```{r}
## Loading the data from Dropbox
s_mobile <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/s_mobile.rds"))
```

```{r}
library(tidyverse)
library(caret)
library(gbm)
```


```{r}
train = s_mobile %>%
  filter(training == 1)
test = s_mobile %>%
  filter(training == 0)
respresentative = s_mobile %>%
  filter(representative == 1)
```

```{r fig.width = 7, fig.height = 35, dpi = 144}
visualize(
  s_mobile, 
  xvar = c(
    "churn", "changer", "changem", "revenue", "mou", "overage", 
    "roam", "conference", "months", "uniqsubs", "custcare", 
    "retcalls", "dropvce", "eqpdays", "refurb", "smartphone", 
    "highcreditr", "mcycle", "car", "travel"
  ), 
  type = "dist", 
  custom = FALSE
)
```

revenue, mou, overage, months, custcare, dropvce

```{r}
## transform variable
train <- mutate_ext(train, .vars = vars(revenue, mou, overage, months, custcare, dropvce), .funs = funs(log), .ext = "_ln")
## transform variable
test <- mutate_ext(test, .vars = vars(revenue, mou, overage, months, custcare, dropvce), .funs = funs(log), .ext = "_ln")
```

```{r}
cm <- function(dat, vars){
  
  cm_df <- as.data.frame(matrix(NA, ncol = 4, nrow = length(vars)))
  colnames(cm_df) <- c("var", "acc", "tpr", "auc")
  
  for (i in 1:length(vars)){
    
    var <- vars[i]
    probs <- pull(dat, !!var)
    resp <- pull(dat, "churn")
    
    pred <- ifelse(pull(dat, !!var) > 0.5, "yes", "no")
    
    acc <- sum(resp == pred)/nrow(dat)
    tpr <- sum(resp == "yes" & pred == "yes")/sum(resp == "yes")
    auc <- ModelMetrics::auc(ifelse(resp=="yes",1,0), probs)

    cm_vec <- c(var, acc, tpr, auc)
    cm_df[i,] <- cm_vec
  }
  return(cm_df)
}

```

stepwise&standardize
```{r}
result <- logistic(
  train, 
  rvar = "churn", 
  evar = c(
    "changer", "changem", "revenue", "mou", "overage", "roam", 
    "conference", "uniqsubs", "custcare", "retcalls", "dropvce", 
    "eqpdays", "refurb", "smartphone", "highcreditr", "mcycle", 
    "car", "travel", "region", "occupation", "months_ln"
  ), 
  lev = "yes", 
  check = c("standardize", "stepwise-backward")
)
summary(result, sum_check = "vif")
```
manually select significant variables based on stepwise model and standardize
```{r}
result <- logistic(
  train, 
  rvar = "churn", 
  evar = c(
    "changem", "mou", "overage", "roam", "uniqsubs","retcalls", "dropvce", 
    "eqpdays", "refurb", "highcreditr", "mcycle", 
    "travel", "region", "occupation", "months_ln"
  ), 
  lev = "yes", 
  check = "standardize"
)
summary(result, sum_check = "vif")
```




```{r}
pred <- predict(result, pred_data = test)
print(pred, n = 10)
test <- store(test, pred, name = "pred_logit")
```

Neural Network
```{r}
## create vector storing paremeters. 
size = seq(1,10,1)
decay = c(0.1,0.5)

## create grid search matrix
params <- expand.grid(size,decay)
value <- matrix(nrow = 20,ncol = 2)
#params$auc <- NA
colnames(params) <- c("size",'decay')

auc <- c()
accuracy <- c()
for (i in 1:nrow(params)){
 
  result <- nn(
  train, 
  rvar = "churn", 
  evar = c(
    "changer", "changem", "revenue", "mou", "overage", "roam", 
    "conference", "uniqsubs", "custcare", "retcalls", "dropvce", 
    "eqpdays", "refurb", "smartphone", "highcreditr", "mcycle", 
    "car", "travel", "region", "occupation", "months"), 
  lev = "yes", 
  size = params[i,1], 
  decay = params[i,2], 
  seed = 1234
)
  pred <- predict(result, pred_data = test)
  print(i)
  test <- store(test, pred, name = name_)
  
  auc_ <- cm(test,name_)$auc
  tpr_ <- cm(test,name_)$tpr
  value[i,1] = auc_
  value[i,2] = tpr_
  colnames(value) = c('auc','tpr')
}
  params = cbind(data.frame(params),value)

saveRDS(params, "nn_tune.rds")

```

```{r} 
nn_tune <-readRDS("nn_tune.rds")
nn_tune %>% 
  arrange(tpr)
```
When size = 4 and decay = 0.1, we get the highest tpr
```{r}
nn_result <- nn(
  train, 
  rvar = "churn", 
  evar = c(
    "changer", "changem", "revenue", "mou", "overage", "roam", 
    "conference", "uniqsubs", "custcare", "retcalls", "dropvce", 
    "eqpdays", "refurb", "smartphone", "highcreditr", "mcycle", 
    "car", "travel", "region", "occupation", "months"),  
  lev = "yes", 
  size = 4, 
  decay = 0.1, 
  seed = 1234
)
nn_pred <- predict(nn_result, pred_data = test)
test <- store(test, nn_pred, name = "pred_nn")
```



```{r}
## Get columns for gbm model
train_gbm = train[c("churn", "changem", "mou", "overage", "roam", "uniqsubs", "retcalls", "dropvce", "eqpdays", "refurb", "highcreditr", "mcycle", "travel", "region", "occupation", "months")]
test_gbm = test[c("churn", "changem", "mou", "overage", "roam", "uniqsubs", "retcalls", "dropvce", "eqpdays", "refurb", "highcreditr", "mcycle", "travel", "region", "occupation", "months")]
```


```{r eval=FALSE}
# Using caret
caretGrid <- expand.grid(interaction.depth = c(2,4,6), 
                         n.trees = c(100,300),
                         shrinkage = c(0.1,0.01),
                         n.minobsinnode = c(20,40))
trainControl <- trainControl(method="cv", number=6, classProbs = TRUE, summaryFunction = twoClassSummary)

set.seed(123)
gbm_caret <- train(churn ~ ., 
              data=train_gbm, 
              distribution="bernoulli", 
              method="gbm",
              trControl=trainControl, verbose=FALSE,
              tuneGrid=caretGrid, metric = "ROC")

print(gbm_caret)
gbm_caret$results
gbm_caret$bestTune

saveRDS(gbm_caret$bestTune, "gbm_best_tune_train1.rds")

saveRDS(gbm_caret$results, "gbm_tune_train1.rds")
```

```{r}
params1 <- readRDS("gbm_tune_train1.rds")
params1 %>% 
  arrange(desc(ROC)) %>% 
  top_n(ROC, n = 10)

best_tune1 <- readRDS("gbm_best_tune_train1.rds")
best_tune1
```

```{r}
train_gbm$churn <- ifelse(train_gbm$churn  == "yes", 1, 0)
test_gbm$churn <- ifelse(test_gbm$churn  == "yes", 1, 0)

best_gbm <- gbm(formula = churn ~ . , 
                  data = train_gbm, 
                  distribution = "bernoulli", 
                  interaction.depth = best_tune1[1,2],
                  shrinkage = best_tune1[1,3],
                  n.trees = best_tune1[1,1],
                  n.minobsinnode = best_tune1[1,4])

gbm_pred <- predict(best_gbm, newdata = test_gbm, type = "response", n.trees = best_tune1[1,1])
test$"pred_gbm" <- gbm_pred
```

```{r}
cols = c('pred_logit','pred_nn','pred_gbm')
cm(test,cols)
```


## logistic final model

```{r}
s_mobile$mou_log <- log(s_mobile$mou + 1)
train_val <- s_mobile %>%
  filter(representative == 0)%>%
  mutate(cweight = ifelse(churn == 'yes', 1L, 49L))

representative <- s_mobile %>%
  filter(representative == 1)
```

```{r}
result <- logistic(
  train_val, 
  rvar = "churn", 
  evar = c(
    "changer", "changem", "revenue", "overage", "roam", "conference", 
    "months", "uniqsubs", "retcalls", "dropvce", 
    "eqpdays", "refurb",  "highcreditr", "mcycle", 
   "travel", "region", "occupation", "mou_log"
  ), 
  lev = "yes", 
  wts = "cweight", 
  check = "standardize"
)
summary(result)
pred <- predict(result, pred_data = representative)
plot(result, plots = "coef", custom = FALSE)
#print(pred, n = 10)
representative <- store(representative, pred, name = "pred_logit")
```


```{r}
coef_table  <- write.coeff(result, sort = TRUE,intercept = FALSE) %>%
  select(label,OR,p.value,importance)

coef_table

coef_table %>%
  ggplot(aes(x = fct_reorder(label, importance), y = importance))+
  geom_bar(stat = 'identity') +
  coord_flip()+
  geom_hline(yintercept = 1,linetype = 'dotted',color = 'red') +
  labs(x = 'variables')
```

eqpdays - statistically speaking, 2 standard deviations increase in eqpdays, equivalent to 
`r round(2 * sd(train_val$eqpdays), 2)` days, would increase the odds of churning by 80.6%, keeping everything else constant. This is probably because that the longer the customer owning the current handset, the more the customer would desire to change to a new handset. As it is common that instead of buying a new handset from a mobile retailer, people may go to a carrier and get a much cheaper price of the same handset with signing several years of contract with the carrier. So the probability of churning increases when a customer wants to change to a new handset.

explore the feature "eqpdays":
```{r}
summary(train_val$eqpdays)

visualize(
  s_mobile, 
  xvar = "eqpdays", 
  type = "dist", 
  custom = FALSE
)

eqpdays_c = train_val %>%
  select(churn,eqpdays)
```

option 1:
```{r}
eqpdays_c = eqpdays_c %>%
  mutate(category1 = ifelse(eqpdays<=150,'low',ifelse(eqpdays<=500,'medium','high')))

table(eqpdays_c$churn,eqpdays_c$category1)
sum(eqpdays_c$eqpdays>500)/nrow(eqpdays_c)

```

option 2 (better):
```{r}
eqpdays_c = eqpdays_c %>%
  mutate(category2 = ifelse(eqpdays<=150,'low',ifelse(eqpdays<=700,'medium','high')))

table(eqpdays_c$churn,eqpdays_c$category2)

sum(eqpdays_c$eqpdays>700)/nrow(eqpdays_c)
```
option 3 (optimal):
```{r}
eqpdays_c = eqpdays_c %>%
  mutate(category3 = ifelse(eqpdays<=100,'low',ifelse(eqpdays<=700,'medium','high')))

table(eqpdays_c$churn,eqpdays_c$category3)

sum(eqpdays_c$eqpdays<=100)/nrow(eqpdays_c)
```
After several attempts, the high group has a raltively high churn rate, while low group has a relatively low churn rate. Medium group has the similar amount of people churning and staying.

```{r}
result <- logistic(
  train_val, 
  rvar = "churn", 
  evar = c(
    "eqpdays"
  ), 
  lev = "yes", 
  check = c("standardize")
)
summary(result)
```




possible way of reducing the churning rate regarding feature "eqpdays":

