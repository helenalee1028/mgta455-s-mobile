---
title: "Laura"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(
  width = 250,
  scipen = 100,
  max.print = 5000,
  stringsAsFactors = FALSE,
  digits = 6
)

## load radiant packages if neededi
if (!exists("r_environment")) library(radiant)
```

```{r}
## Loading the data from Dropbox
s_mobile <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/s_mobile.rds"))
```

```{r include = FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(forcats)
library(forecast)
library(ModelMetrics)
```

```{r}
# log transformation for 'mou' variable
s_mobile$mou_log <- log(s_mobile$mou + 1)

# split into training, validation and representative data
train <- s_mobile %>% 
  filter(training == 1)

validation <- s_mobile %>% 
  filter(training == 0)

representative <- s_mobile %>% 
  filter(representative == 1)
```

### Question 1
#### Logistic Regression

We hand picked the variables whose coefficients are statistically significant at 10% level. Below is model summary. 

```{r warning=FALSE}
logit_result <- logistic(
  train, 
  rvar = "churn", 
  evar = c(
    "changer", "changem", "overage", "roam", "conference", 
    "months", "uniqsubs", "retcalls", "dropvce", "eqpdays", 
    "refurb", "highcreditr", "mcycle", "travel", "region", "occupation", 
    "mou_log", "revenue"
  ), 
  lev = "yes", 
  check = "standardize"
)

summary(logit_result)


# predict on validation data for further comparison

pred_logit <- predict(logit_result, pred_data = validation)$Prediction
```

#### Neural Network

In this part, we tune neural network model by iterating over size 3 to 10, with decay rate from 0.1 to 0.8. The highest validation AUC is achieved by size 10 and decay rate of 0.1. 

```{r eval = FALSE}
### tune neural network
params <- expand.grid(size = seq(3,10,1), decay = seq(0.1, 0.8, 0.1))
params$auc <- NA

real_churn <- ifelse(validation$churn == "yes", 1, 0)
for (i in 1:nrow(params)){
  result <- nn(
    train, 
    rvar = "churn", 
    evar = c(
      "changer", "changem", "revenue", "mou", "overage", "roam", 
      "conference", "months", "uniqsubs", "custcare", "retcalls", 
      "dropvce", "eqpdays", "refurb", "smartphone", "highcreditr", 
      "mcycle", "car", "travel", "region", "occupation"
    ), 
    lev = "yes", 
    size = params[i,1], 
    decay = params[i,2], 
    seed = 1234
  )
  
  pred <- predict(result, pred_data = validation)$Prediction
  
  auc <- ModelMetrics::auc(real_churn, pred)
  params[i,3] <- auc
  
}

saveRDS(params, 'nn_tune.rds')
```

```{r}
nn_tune1 <- readRDS("nn_tune1.rds")
pos <- which.max(nn_tune1$auc)

result <- nn(
    train, 
    rvar = "churn", 
    evar = c(
      "changer", "changem", "revenue", "mou", "overage", "roam", 
      "conference", "months", "uniqsubs", "custcare", "retcalls", 
      "dropvce", "eqpdays", "refurb", "smartphone", "highcreditr", 
      "mcycle", "car", "travel", "region", "occupation"
    ), 
    lev = "yes", 
    size = nn_tune1[pos,1], 
    decay = nn_tune1[pos,2], 
    seed = 1234
  )

pred_nn <- predict(result, pred_data = validation)$Prediction
```

#### Evaluate Models

Comparing prediction accuracy, AUC and true positive rate, we can see that neural network outperforms logistic regression in terms of accuracy and AUC, but underperforms in true positive rate. Since the performance difference isn't substantial, and the key purpose of our analysis is to locate the main drivers for churn rate, we'll use logistic regression for our further analysis.  

```{r}
# define evaluation function
acc <- function(dat, vars){
  
  cm_df <- as.data.frame(matrix(NA, ncol = 4, nrow = length(vars)))
  colnames(cm_df) <- c("var", "acc", "tpr", "auc")
  
  for (i in 1:length(vars)){
    
    var <- vars[i]
    probs <- pull(dat, !!var)
    churn <- pull(dat, "churn")
    
    pred <- ifelse(pull(dat, !!var) > 0.5, "yes", "no")
    
    acc <- sum(churn == pred)/nrow(dat)
    tpr <- sum(churn == "yes" & pred == "yes")/sum(churn == "yes")
    auc <- ModelMetrics::auc(ifelse(churn=="yes",1,0), probs)

    cm_vec <- c(var, acc, tpr, auc)
    cm_df[i,] <- cm_vec
    
    cm_df[2:4] <- lapply(cm_df[2:4], as.numeric)
  }
  return(cm_df)
}

vars <- c("pred_logit", "pred_nn")
eval_df <- validation %>% 
  select(churn) %>% 
  mutate(pred_logit = pred_logit, pred_nn = pred_nn)

acc(eval_df, vars)
```

### Question 2 - Key Driver Analysis

After choosing the suitable model, we train on the entire dataset (training + validation) and extend key drivers analysis. 

```{r fig.width = 7, fig.height = 8.18, dpi = 144}

entire_train <- rbind(train, validation)
result <- logistic(
  entire_train, 
  rvar = "churn", 
  evar = c(
    "changer", "changem", "overage", "roam", "conference", 
    "months", "uniqsubs", "retcalls", "dropvce", "eqpdays", 
    "refurb", "highcreditr", "mcycle", "travel", "region", "occupation", 
    "mou_log", "revenue"
  ), 
  lev = "yes", 
  check = "standardize"
)
plot(result, plots = "coef", custom = FALSE)

```

```{r}
coef_table <- result %>% write.coeff(intercept = FALSE) %>%
  format_df(dec = 4) %>% 
  select(label, OR, coefficient, std.error, z.value, p.value) %>%
  mutate_at(vars(label), list(as.factor)) %>% 
  mutate_at(vars(-label), list(as.numeric)) %>% 
  mutate(importance = ifelse(OR >1, OR, 1.0/OR)) %>% 
  arrange(desc(importance))

coef_table
```

```{r}
coef_table %>% 
  ggplot(aes(x = fct_reorder(label, importance), y = importance)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_hline(yintercept = 1.0, linetype = 'dotted', color = 'red')+
  labs(main = "Variable Importance in Churn Rate Prediction", 
       x = "variables",
       y = 'importance')
  
```

Key Driver Analysis:

1. overage - statistically speaking, 2 standard deviations increase in overage, equivalent to 
`r round(2 * sd(entire_train$overage), 2)` minutes, would increase the odds of churning by 92%, keeping everything else constant. This is probably because customers find the current plan unable to satisfy their needs and hence considering switching to other providers. 

2. mou_log - similarly, 2 standard deviations increase in log transformed 'mean monthly minutes of use', which is equivalent to around 18 minutes, would decrease the odds of customer churning by approximately 40%. 


### Question 3 - Proactive strategy proposals

Based on drivers analysis, we come up with below actions and incentives for proactive churn management. 

1. Promote plans with larger data usage to customers whose consumption constantly exceed monthly limit;



### Question 4 - Quantify the impact on churn probability

```{r}
# use model without standard deviations for prediction
logit_model <- glm(churn == "yes" ~ changer+changem+mou_log+overage+roam+
                     conference+months+uniqsubs+retcalls+dropvce+eqpdays+
                     refurb+highcreditr+mcycle+travel+region+occupation+revenue, 
                   data = entire_train, family = "binomial")

## adjust to actual churn rate of 2%
adjust_prob <- function(pred_prob){
  probs_adj <- pred_prob / (pred_prob + (1 - pred_prob)*0.98/0.02)
  probs_adj
}

pred_prob <- predict(logit_model, newdata = representative, 
                     type = "response")
pred_prob_adj <- adjust_prob(pred_prob) # adjust to actual level
baseline <- mean(pred_prob_adj)


pred_prob_adj <- adjust_prob(pred_prob)
print(paste0("The churn rate without proactive incentive is ", format_nr(mean(pred_prob_adj), perc = T), "."))

# we target to reduce below median overage without incurring substantial rise in cost
target_usage <- median(entire_train$overage)
representative_overage <- representative %>% 
  mutate(overage = ifelse(overage > target_usage, target_usage, overage))
pred_overage <- predict(logit_model, newdata = representative_overage, type = "response") %>% 
  adjust_prob()

overage_churn <- mean(pred_overage)

```

1. The baseline churn rate is `r format_nr(baseline,perc = T)`. If we can manage to reduce overage below 43 minutes, which is the current median overage time, we can reduce the average churn rate to `r format_nr(overage_churn, perc = T)`. The reason why we are not seeking to eliminate overage altogether is that it may incur high maintenance cost to keep up with the increased data flow. For example, if we offer unlimited plans, customers may abuse data usage and S-mobile may need extra capital in infrastructure maintenance to keep up with the increased service request. 


```{r}
hist(representative$eqpdays)

# can we cut within one year

representative_equip <- representative %>% 
  mutate(eqpdays = ifelse(eqpdays > 365, 365, eqpdays))

pred_eqpdays <- predict(logit_model, newdata = representative_equip, type = "response") %>% 
  adjust_prob()

mean(pred_eqpdays)
```

### Question 5 - Customer Target


### Question 6 - Evaluate the economics

To evaluate the economic impact on 5-year profitability, we made following assumptions for customer lifetime value calculation. 

1. Cost - there are 2 main components for costs, fixed and variable. The former is mainly for infrastructure and general admin maintenance and estimated to be $8 for each customer each month. The latter is variable cost which varies with service requests and we assume a cost rate of 40%;

2. Annual Discount Rate - we assume it to be 12%, which is a reasonable assumption for a company's annual rate of return;

3. Revenue arrives at the end of each month;

4. Customers cancel their plans at midst of an month, which isn't effective til the end of the month. 

```{r}
# initial setup for CLV calculation
n <- 5 * 12
discount <- 0.12
discount_period <- c(1:n)

revenue <- rep(mean(representative$revenue), n)
fixed_cost <- rep(8, n)
initial_variable <- 0.4

monthly_discount <- (1 + discount)^(1/12) - 1


# define clv calculation function
cal_clv <- function(churn_rate, cost_rate){
  total_cost <- fixed_cost + cost_rate * revenue
  profits <- revenue - total_cost
  
  remain_customer <- rep(1,n)
  for (i in 1:(length(remain_customer)-1)){
    remain_customer[i+1] <- remain_customer[i] * (1 - churn_rate)
  }
  
  profits_churn <- profits*remain_customer
  
  profits_pv <- profits_churn/((1 + monthly_discount)^discount_period)
  
  cum_profits <- cumsum(profits_pv)
  
  clv <- cum_profits[60]
}

```

For an average customer, the monthly revenue is expected to be `r format_nr(mean(representative$revenue),'$', dec = 2)`. After reducing the overage to 43 minutes, the average churn rate is expected to be `r format_nr(overage_churn, perc = T)`. However, to accomodate higher data usage, we may expect increase in variable costs as more resources need to be allocated. So in our calculation, we assume the variable cost rate to be 42%, a reasonable rise from our initial assumption of 40%.

```{r}
base_clv <- cal_clv(baseline, initial_variable)
overage_clv <- cal_clv(overage_churn, 0.42)
```

```{r}
# define function to minimize the difference
cal_diff <- function(churn_rate, cost_rate){
  clv <- cal_clv(churn_rate, cost_rate) # clv after incentive
  diff <- abs(clv - base_clv)
  return(diff)
}

optimize_result <- optimize(cal_diff, interval = c(0.4, 0.5), maximum = FALSE, churn_rate = overage_churn) # find the cost rate that will minimize the difference between clv after intervention and baseline clv
max_cost_rate <- optimize_result$minimum

```

Based on our calculation, we expect an increase of `r format_nr(overage_clv - base_clv, '$')` in average CLV, based on our assumptions. To be more specific, we can spend up to `r format_nr(max_cost_rate, perc = T)` of revenues as variable cost to allow this strategy. Any cost beyond that is not worthwhile. 
